{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5de837",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# 목차\n",
    "## Step 1. 데이터 다운로드\n",
    "## Step 2. 데이터 읽어오기\n",
    "## Step 3. 데이터 정제\n",
    "## Step 4. 평가 데이터셋 분리\n",
    "## Step 5. 인공지능 만들기\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b63ec",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "### Cloud shell에 아래의 명령어 입력하기\n",
    "### mkdir -p ~/aiffel/lyricist/models\n",
    "###  ln -s ~/data ~/aiffel/lyricist/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea17bb7",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502502b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/Exploration/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeffaac",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c98db1",
   "metadata": {},
   "source": [
    "### shakespeare.txt의 경우, txt파일을 열어보면\n",
    "### - First Citizen:\n",
    "### - Before we proceed any further, hear me speak.\n",
    "### -  \n",
    "### - All:\n",
    "### - Speak, speak.\n",
    "### 이렇게 되어 있으나 lyrics 파일의 경우, 예를 들어 adele.txt 파일을 열어보면\n",
    "### - Looking for some education\n",
    "### - Made my way into the night\n",
    "### - All that bullshit conversation\n",
    "### - 이렇게 되어 있기 때문에 '문장의 끝이 :'로 끝나거나 '공백'인 문장을 제외할 필요가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14bc2e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백을 없애기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백을 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백을 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가하기\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab21a067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 원치 않는 문장 넘기기\n",
    "    if len(sentence) == 0: continue\n",
    "    # 정제하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    if len((preprocessed_sentence).split()) > 15 : continue #토큰 15개 넘는 것은 제외하기\n",
    "    \n",
    "    \n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과 10개만 확인하기\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7455a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2971 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  117 ...    0    0    0]\n",
      " [   2  258  195 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd3c03c12b0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 단어장 크기 12,000개 이상\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e88b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6269]\n",
      " [   2   15 2971  872    5    8   11 5747    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0eb75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42030eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6269    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6269    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313d47a",
   "metadata": {},
   "source": [
    "# Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74d0668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train 개수:  124981 , dec_train 개수:  124981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                    tgt_input, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=7)\n",
    "print('enc_train 개수: ', len(enc_train),', dec_train 개수: ', len(dec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55b779c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbce2f",
   "metadata": {},
   "source": [
    "# Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de554847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df5d9f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-8.57947598e-05,  7.78614194e-05, -1.95558780e-04, ...,\n",
       "         -5.97379876e-05, -9.27443980e-05,  9.59210593e-05],\n",
       "        [ 2.55299441e-04,  3.67578410e-04,  1.40814700e-05, ...,\n",
       "         -4.80417584e-05, -1.31288383e-04,  9.36184442e-05],\n",
       "        [ 1.98625115e-04,  6.37353631e-04,  2.56465573e-04, ...,\n",
       "         -1.04803643e-04, -3.13693134e-04,  9.51179172e-05],\n",
       "        ...,\n",
       "        [-1.52474199e-03,  8.56081897e-04, -5.21429640e-04, ...,\n",
       "          1.01702230e-03, -9.59177443e-04,  8.17397784e-04],\n",
       "        [-1.55256141e-03,  9.48905945e-04, -8.19268287e-04, ...,\n",
       "          1.09451299e-03, -9.63981322e-04,  1.04702159e-03],\n",
       "        [-1.53897342e-03,  1.04870705e-03, -1.07510388e-03, ...,\n",
       "          1.15974073e-03, -9.64918756e-04,  1.26489787e-03]],\n",
       "\n",
       "       [[-8.57947598e-05,  7.78614194e-05, -1.95558780e-04, ...,\n",
       "         -5.97379876e-05, -9.27443980e-05,  9.59210593e-05],\n",
       "        [ 1.35312730e-04,  3.15372192e-04, -3.43853200e-04, ...,\n",
       "         -5.45220537e-05, -2.36182226e-04,  3.86299798e-04],\n",
       "        [ 2.09351172e-04,  4.44341829e-04, -2.89761898e-04, ...,\n",
       "          2.58625136e-04, -1.91165527e-04,  5.26409596e-04],\n",
       "        ...,\n",
       "        [-7.81676150e-04,  6.11537907e-05,  6.42497151e-04, ...,\n",
       "          6.67961081e-04, -2.99962616e-04,  4.12918365e-04],\n",
       "        [-1.14704715e-03,  1.77835700e-05,  2.72870937e-04, ...,\n",
       "          7.31112901e-04, -4.78581933e-04,  5.34469204e-04],\n",
       "        [-1.42638339e-03,  4.53259854e-05, -1.27911786e-04, ...,\n",
       "          7.79842492e-04, -6.17260346e-04,  7.04040693e-04]],\n",
       "\n",
       "       [[-8.57947598e-05,  7.78614194e-05, -1.95558780e-04, ...,\n",
       "         -5.97379876e-05, -9.27443980e-05,  9.59210593e-05],\n",
       "        [-1.19246550e-04,  4.07215237e-04, -3.35959659e-04, ...,\n",
       "          1.08277891e-04, -3.24051245e-04,  1.59211573e-04],\n",
       "        [-2.76574545e-04,  1.95808505e-04, -2.62849237e-04, ...,\n",
       "          5.97325961e-05, -4.83133103e-04, -4.35427792e-05],\n",
       "        ...,\n",
       "        [-6.31802250e-04,  7.01544923e-04,  7.47628743e-04, ...,\n",
       "          9.06681889e-05,  4.01441430e-05, -8.73982208e-04],\n",
       "        [-9.11991345e-04,  7.09542132e-04,  6.29236340e-04, ...,\n",
       "          3.16134538e-04, -1.58713738e-04, -8.34257575e-04],\n",
       "        [-1.16602133e-03,  7.15836068e-04,  3.84234387e-04, ...,\n",
       "          5.07128192e-04, -3.65472457e-04, -6.96032017e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.57947598e-05,  7.78614194e-05, -1.95558780e-04, ...,\n",
       "         -5.97379876e-05, -9.27443980e-05,  9.59210593e-05],\n",
       "        [-3.08359624e-04,  1.09421118e-04, -2.00937808e-04, ...,\n",
       "         -1.18035285e-04, -1.66143815e-04,  1.23234320e-04],\n",
       "        [-3.98056087e-04,  8.50744764e-05, -4.00776800e-04, ...,\n",
       "         -6.80105441e-05,  1.21800789e-04,  1.47351268e-04],\n",
       "        ...,\n",
       "        [ 7.64001568e-04,  1.00765971e-03, -5.63889160e-04, ...,\n",
       "          1.06040665e-04,  2.19244786e-04, -7.97684072e-04],\n",
       "        [ 6.81054837e-04,  1.29146979e-03, -3.42798769e-04, ...,\n",
       "          2.49384262e-04, -5.68909636e-05, -3.39879625e-04],\n",
       "        [ 6.01302949e-04,  1.32771814e-03, -1.52658802e-04, ...,\n",
       "          3.46109329e-04, -1.99163886e-04, -1.00400095e-04]],\n",
       "\n",
       "       [[-8.57947598e-05,  7.78614194e-05, -1.95558780e-04, ...,\n",
       "         -5.97379876e-05, -9.27443980e-05,  9.59210593e-05],\n",
       "        [-1.78522998e-04,  5.15259395e-04, -2.76882231e-04, ...,\n",
       "          6.78692231e-05, -1.08147702e-04,  3.54848540e-04],\n",
       "        [-4.43683763e-04,  8.87231261e-04,  2.16751632e-05, ...,\n",
       "          3.31170158e-04,  3.92629299e-05,  4.16703842e-04],\n",
       "        ...,\n",
       "        [-1.58498657e-03,  1.31265714e-03, -1.63625611e-03, ...,\n",
       "          1.31605065e-03, -1.00013486e-03,  1.33762148e-03],\n",
       "        [-1.50578644e-03,  1.35872304e-03, -1.76840776e-03, ...,\n",
       "          1.34617754e-03, -1.01215497e-03,  1.52317027e-03],\n",
       "        [-1.41270936e-03,  1.40733714e-03, -1.87200040e-03, ...,\n",
       "          1.37395342e-03, -1.01604802e-03,  1.69379683e-03]],\n",
       "\n",
       "       [[-8.57947598e-05,  7.78614194e-05, -1.95558780e-04, ...,\n",
       "         -5.97379876e-05, -9.27443980e-05,  9.59210593e-05],\n",
       "        [ 1.94865206e-05,  2.26561635e-04, -1.51442437e-04, ...,\n",
       "         -1.12441106e-04, -4.63333359e-04, -1.22597539e-05],\n",
       "        [ 9.95851005e-05,  5.85866452e-04,  4.94459418e-05, ...,\n",
       "         -3.44385800e-04, -6.26556808e-04, -7.85974262e-05],\n",
       "        ...,\n",
       "        [-8.78712977e-04,  1.11349486e-03, -3.30941664e-04, ...,\n",
       "          2.41529706e-04, -6.98994030e-04,  6.08448172e-04],\n",
       "        [-1.04486267e-03,  1.10793661e-03, -6.81479229e-04, ...,\n",
       "          4.56054375e-04, -8.00667854e-04,  7.94631604e-04],\n",
       "        [-1.15831371e-03,  1.12862175e-03, -9.99364653e-04, ...,\n",
       "          6.43216539e-04, -8.73853336e-04,  9.98586416e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 한 배치만 불러오기.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde5eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  18466976  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  32780704  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24302025  \n",
      "=================================================================\n",
      "Total params: 78,621,961\n",
      "Trainable params: 78,621,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc417bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7812/7812 [==============================] - 596s 76ms/step - loss: 2.9207 - val_loss: 2.6786\n",
      "Epoch 2/4\n",
      "7812/7812 [==============================] - 598s 77ms/step - loss: 2.4262 - val_loss: 2.4650\n",
      "Epoch 3/4\n",
      "7812/7812 [==============================] - 599s 77ms/step - loss: 2.0402 - val_loss: 2.3470\n",
      "Epoch 4/4\n",
      "7812/7812 [==============================] - 599s 77ms/step - loss: 1.7286 - val_loss: 2.2988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd3a85f5910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=4, validation_data=(enc_val,dec_val), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ecde569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d383444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so , <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31564407",
   "metadata": {},
   "source": [
    "# REPROT\n",
    "### 훈련을 시키는데 너무 많은 시간이 걸려서 많이 돌려보지 못해서 아쉽다.\n",
    "### 처음에 만들었을때 val_loss가 나오지 않아서 당황스러웠는데 코드를 덜 작성한 탓이었다..\n",
    "### 이후에 '거울아 거울아~' 이 노드를 보면서 참고해서 수정을 했다.\n",
    "### 이번에는 아람님의 도움을 받았다.\n",
    "### 학습을 하다가 val_loss가 2.3까지 내려갔다가 계속 훈련하니 2.4로 다시 올라가는 경우가 있었는데, 이 부분은 batch_size=16로 줄이고, epochs=4로 줄이는 것으로 해결되었다.\n",
    "### i love you so 라는 문장도 해석해보면 전혀 이상한 문장도 아니라서 이번 프로젝트도 잘 해결되었다! (찾아보니 해당 제목의 노래도 있었다!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c94fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
